---
title: 'Appendix: Code Listings'
output:
  word_document: default
  pdf_document: default
  html_notebook: default
---

# Project Structure
######   |--- data_analysis.rmd
######   |--- myutils.R
######   |    |--- BetterPairs()
######   |    |--- ClassificationReport()
######   |    |--- LdaQdaDecisionBoundary()
######   |    |--- KnnDecisionBoundary()
######   |    |--- PlotTree()
######   |    |--- PredictRegsubsets()
######   |    |--- MSE()
######   |--- traningdata201617.csv


# Initial Exploration of Data
```{r}
# Initialisation
# Clear workspace
rm(list = ls())
# Set working directory
path <- "/Users/songer1993/OneDrive/L120_Principles_of_Data_Science/practical"
setwd(path)
```

```{r}
# set default seed number in this analysis
seed.num <- 20170117
```

```{r results='hide', message=FALSE, warning=FALSE}
# Load required packages
# Regression
library(MASS) # 7.3-45
library(glmnet) # 2.0-5
library(pls) # 2.6-0
# Classification
library(class) # 7.3-14
library(caret) # 6.0-73
# Tidy / Transform
library(tidyverse) # 1.0.0
library(broom) # 0.4.1
library(reshape) # 0.8.6
# Exploration, Visualisation and Metrcis
library(tree) # 1.0-37
library(car) # 2.1-4
library(gridExtra) # 2.2.1
# Validation
library(boot) # 1.3-18
# Variable Selection
library(leaps) # 3.0
# Others
source("myutils.R") # some helper functions

```

```{r}
# Check R version,
# packages loaded and their versions
sessionInfo()
```

```{r}
# Import data
filename <- "trainingdata201617.csv"
data <- tbl_df(read.csv(filename))
```

```{r}
print(data)
str(data)
```

```{r}
# Check NA values in each experiments
# and identify dependent variables
data %>%
    group_by(label) %>%
    summarise(
        count = n(),
        Y = sum(!is.na(Y)),
        X1 = sum(!is.na(X1)),
        X2 = sum(!is.na(X2)),
        X3 = sum(!is.na(X3)),
        X4 = sum(!is.na(X4)),
        X5 = sum(!is.na(X5)),
        X6 = sum(!is.na(X6)),
        X7 = sum(!is.na(X7)),
        X8 = sum(!is.na(X8)),
        X9 = sum(!is.na(X9))
    ) %>%
    print()
```


# Experiment alfa
```{r}
# Retrive alfa data subset
# and keep relevant columns
data.alfa <- data %>% 
    filter(label == "alfa") %>%
    select(-c(id, label))
str(data.alfa)
```


```{r}
# Initial graphical data subset exploration
BetterPairs(data.alfa)
PlotTree(data.alfa)
```

```{r}
# By inspection, X3 and X7 are the most important variables
# Fit a linear regression model using them first
lm.alfa <- lm(Y~X3+X7, data = data.alfa)
ShortSummary(lm.alfa)
par(mfrow = c(2, 2))
plot(lm.alfa)
```

```{r}
# Split data into training and testing subset for validation
set.seed(seed.num)
train <- sample(1:nrow(data.alfa), nrow(data.alfa)/2)
test <- (-train)
# divide by training and testing
training <- data.alfa[train, ]
testing <- data.alfa[test, ]
# divide by response and predictors
X <- model.matrix(Y ~ ., data = data.alfa)[, -1]
Y <- data.alfa$Y
```


```{r}
# Subset Selection Methods Using 10-fold Cross Validation

k <- 10 # k for K-fold CV
p <- 9 # number of predictors

# Randomly create k folds from data 
set.seed(seed.num)
folds <- sample(1:k, nrow(training), replace=TRUE)

# Initialise validation MSE matrix
cv.errors.best <- matrix(NA, k, p, dimnames=list(NULL, paste(1:p)))
cv.errors.fwd <- matrix(NA, k, p, dimnames=list(NULL, paste(1:p)))
cv.errors.bwd <- matrix(NA, k, p, dimnames=list(NULL, paste(1:p)))

# Loop k * p times to calculate validation MSE
# for best, forward and backward selection methods
for(j in 1:k){
  best.fit=regsubsets(Y ~ ., data = training[folds != j, ], nvmax= p)
  fwd.fit=regsubsets(Y ~ ., data = training[folds != j, ], nvmax= p,
                     method = "forward")
  bwd.fit=regsubsets(Y ~ ., data = training[folds != j, ], nvmax= p,
                     method = "backward")
  for(i in 1:p){
    pred <- PredictRegsubsets(best.fit, training[folds == j,], id = i)
    cv.errors.best[j, i] <- mean( (training$Y[folds == j] - pred) ^ 2)
    pred <- PredictRegsubsets(fwd.fit, training[folds == j,], id = i)
    cv.errors.fwd[j, i] <- mean( (training$Y[folds == j] - pred) ^ 2)
    pred <- PredictRegsubsets(bwd.fit, training[folds == j,], id = i)
    cv.errors.bwd[j, i] <- mean( (training$Y[folds == j] - pred) ^ 2)
    }
}

# Calculate average validation MSE and find number of predictors with
# smallest average validation MSE for best, forward and backward 
# selection methods
mean.cv.errors.best <- apply(cv.errors.best,2,mean)
p.best <- which.min(mean.cv.errors.best)
mean.cv.errors.fwd <- apply(cv.errors.fwd,2,mean)
p.fwd <- which.min(mean.cv.errors.fwd)
mean.cv.errors.bwd <- apply(cv.errors.bwd,2,mean)
p.bwd <- which.min(mean.cv.errors.bwd)

# Perform best subset selection on the full training subset
# and print coefficients
reg.best <- regsubsets(Y ~ ., data = training, nvmax = p)
coef(reg.best, p.best)
# Predict Y on testing subset and report test MSE
pred <- PredictRegsubsets(reg.best, testing, id = p.best)
MSE.best <- MSE(pred, testing$Y) # 16.79835


# Perform forward stepwise subset selection on the full training subset
# and print coefficients
reg.fwd <- regsubsets(Y ~ ., data = training, nvmax = p, 
                      method = "forward")
coef(reg.fwd, p.fwd)
# Predict Y on testing subset and report test MSE
pred <- PredictRegsubsets(reg.fwd, testing, id = p.fwd)
MSE.fwd <- MSE(pred, testing$Y) # 16.79835

# Perform backward stepwise subset selection on the full training subset
# and print coefficients
reg.bwd <- regsubsets(Y ~ ., data = training, nvmax = p, 
                      method = "backward")
coef(reg.bwd, p.bwd)
# Predict Y on testing subset and report test MSE
pred <- PredictRegsubsets(reg.bwd, testing, id = p.bwd)
MSE.bwd <- MSE(pred, testing$Y) # 16.79835
```

```{r}
# Fit ridge regression model
grid <- 10^seq(10, -2, length = 100)
ridge.model <- glmnet(X[train, ], Y[train], alpha = 0, lambda = grid,
                      thresh = 1e-12)

# 10-fold cross-validation
# to find out best lambda value
set.seed(seed.num)
cv.out <- cv.glmnet(X[train, ], Y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min # 14.44839

# Show coefficients of final ridge regression model
out <- glmnet(X, Y, alpha = 0, lambda = grid)
ridge.coef <- predict(out, type = "coefficients", s = bestlam)
print(ridge.coef)

# Predict Y values on testing data
# and estimate test MSE
ridge.pred <- predict(ridge.model, s = bestlam, newx = X[test, ])
MSE.ridge <- MSE(ridge.pred, Y[test]) # 166.274
```

```{r}
# Fit lasso regression model
lasso.model <- glmnet(X[train, ], Y[train], alpha = 1, lambda = grid,
                      thresh = 1e-12)

# 10-fold cross-validation
# to find out best lambda value
set.seed(seed.num)
cv.out <- cv.glmnet(X[train, ], Y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min # 3.186059

# Show coefficients of final lasso regression model
out <- glmnet(X, Y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)
print(lasso.coef)

# Predict Y values on testing data
# and estimate test MSE
lasso.pred <- predict(lasso.model, s = bestlam, newx = X[test, ])
MSE.lasso <- MSE(lasso.pred, Y[test]) # 30.79602
```

```{r}
# Fit PCR model
set.seed(seed.num)
pcr.fit <- pcr(Y ~ ., data = training, scale = TRUE,
               validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP") #9

# Compute test MSE
pcr.pred <- predict(pcr.fit, X[test, ], ncomp = 9)
MSE.pcr <- MSE(pcr.pred, Y[test]) # 17.88188
```

```{r}
# Fit PLS model
set.seed(seed.num)
pls.fit <- plsr(Y ~ ., data = training, scale = TRUE,
               validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP") #9

# Compute test MSE
pls.pred <- predict(pls.fit, X[test, ], ncomp = 3)
MSE.pls <- MSE(pls.pred, Y[test]) # 18.12962
```

```{r}
# Compute test MSE of the first model
lm.alfa <- lm(Y~X3+X7, data = training)
ShortSummary(lm.alfa)
lm.pred <- predict(lm.alfa, newdata = testing)
MSE.my <- MSE(lm.pred, testing$Y) # 17.06376
```

```{r}
# Plot all model's test MSE
# Construct data frame for ploting
metrics.df <- bind_rows(MSE.best,
                        MSE.fwd,
                        MSE.bwd,
                        MSE.ridge,
                        MSE.lasso,
                        MSE.pcr,
                        MSE.pls,
                        MSE.my)
metrics.df <- transmute(metrics.df, 
                        test.MSE = round(MSE, 2))
metrics.df$model <- c("best", "fwd", "bwd", "ridge", "lasso", "pcr", "pls", "My")
metrics.df<- melt(metrics.df, id.vars='model')

# Plot bar chart
ggplot(metrics.df, aes(x = model, y = value)) +   
    geom_bar(aes(fill = variable), position = "dodge", stat="identity") + 
    geom_text(aes(x = model, y = value, label = value)) +
    ylab("Test MSE")
```

# Experiment bravo
```{r}
# Retrive bravo data subset
# and keep relevant columns
data.bravo <- data %>% 
    filter(label == "bravo") %>% 
    select(Y, X3, X7)
str(data.bravo)
```

```{r}
# Initial subset data exploration
BetterPairs(data.bravo)
PlotTree(data.bravo)
```

```{r}
# Visualise X3 - X7 2D Plane
data.bravo <- mutate(data.bravo, Y.label = as.factor(Y))
ggplot(data.bravo, aes(x = X3, y = X7, colour = Y.label)) + 
    geom_point(data = data.bravo, aes(pch = Y.label,  colour= Y.label))  
```

```{r}
# Split data into training and testing subset for validation
set.seed(seed.num)
train <- sample(1:nrow(data.bravo), nrow(data.bravo)/2)
test <- (-train)
training <- data.bravo[train, ]
testing <- data.bravo[test, ]
testing.actual <- testing$Y.label
training.X <- select(training, X3, X7)
testing.X <- select(testing, X3, X7)
training.Y <- training$Y.label
```

```{r results='hide', message=FALSE, warning=FALSE}
# 10-fold cross validation on logisitic regression
# to find out the best formula explaining Y

# Set max polynomial degree to 10
poly.degree <- 10

# Initialise cross validation error
# for polynomial regression models (up to 10-degree)
cv.error.10 <- rep(0, poly.degree)

# Loop 10 times to fit data into 1-10 degree polynomial models
# and calculate and store 10-fold cross validation results
cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)
set.seed(seed.num)
for(i in 1:poly.degree){
    glm.bravo <- glm(Y ~ polym(X3, X7, degree = i, raw = TRUE), 
                     data = data.bravo, family = "binomial")
    cv.error.10[i] <- cv.glm(data.bravo, glm.bravo, cost, K = 10)$delta[1]
}

# Print the polynomial degree with minimum MSE
best.degree <- which.min(cv.error.10)

# Plot 10-fold cross validation results
qplot(1:length(cv.error.10), cv.error.10) + 
    geom_point(pch = 8) +
    geom_line() +
    xlab("Fitting Polynomial Degree") +
    ylab("Error Rate") +
    annotate("text", x = which.min(cv.error.10), y = min(cv.error.10), 
             label = "O", col = "red")

```

```{r}
# First degree model is simple and its Error Rate
# is comparable to the best degree (4)
# So, fit logistic model using X3 and X7
glm.bravo <- glm(Y.label ~ X3 + X7, data = data.bravo, family = binomial, subset = train)
#summary(glm.bravo)

# Calculate testing data prediction accuracy
testing.probs <- predict(glm.bravo, testing, type = "response")
testing.pred <- rep(0, nrow(testing))
testing.pred[testing.probs > 0.5] <- 1
ClassificationReport(testing.pred, testing.actual)[1:2]
accuracy.glm <- ClassificationReport(testing.pred, testing.actual)[2] # 0.928
```

```{r}
# Fit LDA classifier 
# and calculate testing data prediction accuracy
lda.bravo <- lda(Y.label ~ X3 + X7, data = data.bravo, subset = train)
testing.pred <- predict(lda.bravo, testing)$class
ClassificationReport(testing.pred, testing.actual)[1:2]
accuracy.lda <- ClassificationReport(testing.pred, testing.actual)[2] # 0.928
```

```{r}
# Fit QDA classifier 
# and calculate testing data prediction accuracy
qda.bravo <- qda(Y.label ~ X3 + X7, data = data.bravo, subset = train)
testing.pred <- predict(qda.bravo, testing)$class
ClassificationReport(testing.pred, testing.actual)[1:2]
accuracy.qda <- ClassificationReport(testing.pred, testing.actual)[2] # 0.924
```

```{r}
# Visualise KNN decision boundaries as K increases
g1 <- ggplot(data.bravo, aes(x = X3, y = X7) ) +
    geom_point(data = data.bravo, aes(pch = Y.label,  colour= Y.label)) + 
    KnnDecisionBoundary(training.X, training.Y, k = 1) +
    ggtitle("K = 1")
g2 <- ggplot(data.bravo, aes(x = X3, y = X7) ) +
    geom_point(data = data.bravo, aes(pch = Y.label,  colour= Y.label)) + 
    KnnDecisionBoundary(training.X, training.Y, k = 5) + 
    ggtitle("K = 5")
g3 <- ggplot(data.bravo, aes(x = X3, y = X7) ) +
    geom_point(data = data.bravo, aes(pch = Y.label,  colour= Y.label)) + 
    KnnDecisionBoundary(training.X, training.Y, k = 11) +
    ggtitle("K = 11")
g4 <- ggplot(data.bravo, aes(x = X3, y = X7) ) +
    geom_point(data = data.bravo, aes(pch = Y.label,  colour= Y.label)) + 
    KnnDecisionBoundary(training.X, training.Y, k = 15) +
    ggtitle("K = 15")
grid.arrange(g1, g2, g3, g4, nrow = 2, ncol = 2)
```

```{r}
# 10-fold cross validation on Knn
# to choose the best K 
set.seed(seed.num)
ctrl <- trainControl(method="cv") 
knnFit <- train(Y.label ~ X3 + X7, data = training, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)

# Results of kNN fit
plot(knnFit)
best.K <- knnFit$finalModel$k #35
```

```{r}
knnPredict <- predict(knnFit,newdata = testing)
ClassificationReport(knnPredict, testing.actual)[1:2]
accuracy.knn <- ClassificationReport(testing.pred, testing.actual)[2] # 0.932
accuracy.knn$Accuracy <- 0.932
```

```{r}
# Visualise all decision boundaries
ggplot(data.bravo, aes(x=X3, y=X7) ) +
    geom_point(data = data.bravo, aes(pch = Y.label,  colour= Y.label, alpha = 0.3)) + 
    geom_point(data = training, aes(pch = Y.label,  colour= Y.label))+
    LogisticRegressionDecisionBoundary(model = glm.bravo) +
    LdaQdaDecisionBoundary(data.X = data.bravo[, 2:3], model = lda.bravo, colour = "red") + 
    LdaQdaDecisionBoundary(data.X = data.bravo[, 2:3], model = qda.bravo, colour = "blue") + 
    KnnDecisionBoundary(training.X, training.Y, k = best.K)
```

```{r}
# Plot all model's accuracy
# Construct data frame for ploting
metrics.df <- bind_rows(accuracy.glm,
                        accuracy.lda,
                        accuracy.qda,
                        accuracy.knn)
metrics.df <- transmute(metrics.df, 
                        Accuracy = round(Accuracy, 4))
metrics.df$model <- c("logistic", "LDA", "QDA", "KNN(35)")
metrics.df<- melt(metrics.df, id.vars='model')

# Plot bar chart
ggplot(metrics.df, aes(x = model, y = value)) +   
    geom_bar(aes(fill = variable), position = "dodge", stat="identity") + 
    geom_text(aes(x = model, y = value, label=value)) +
    ylab("Accuracy")
```


# Experiment charlie
```{r}
# Retrive charlie data subset
# and keep relevant columns
data.charlie <- data %>% 
    filter(label == "charlie") %>%
    select(Y, X7)
str(data.charlie)
```

```{r}
# Initial subset data exploration
BetterPairs(data.charlie)
```

```{r}
# Fit a simple linear model 
# and plot residuals for further diagnotics
lm.charlie <- lm(Y ~ X7, data = data.charlie)
ShortSummary(lm.charlie)
par(mfrow = c(2, 2))
plot(lm.charlie)
```


```{r}
# Transform response variable using log
# and refit linear model
lm.charlie.log <- lm(log(Y) ~ X7, data = data.charlie)
ShortSummary(lm.charlie.log)
```

```{r}
# Transform response variable using sqrt
# and refit linear model
lm.charlie.sqrt <- lm(sqrt(Y) ~ X7, data = data.charlie)
ShortSummary(lm.charlie.sqrt)
```

```{r}
# Fit a Weighted Least Squares model with weight = 1/X7 
weights <- 1/(data.charlie$X7)
lm.charlie.weighted <- lm(Y ~ X7, data = data.charlie, weights = weights)
ShortSummary(lm.charlie.weighted)
```

```{r}
# Plot Residuals vs Fitted for each model
par(mfrow = c(2, 2))
plot(lm.charlie, which = 1, main = "Respnse Y\n")
plot(lm.charlie.log, which = 1, main = "Response log(Y)\n")
plot(lm.charlie.sqrt, which = 1, main = "Response sqrt(Y)\n")
plot(lm.charlie.weighted, which = 1, main = "Weighted by 1/X7\n")

# Plot Residuals vs Leverage for each model
par(mfrow = c(2, 2))
plot(lm.charlie, which = 5, main = "Respnse Y\n")
plot(lm.charlie.log, which = 5, main = "Response log(Y)\n")
plot(lm.charlie.sqrt, which = 5, main = "Response sqrt(Y)\n")
plot(lm.charlie.weighted, which = 5, main = "Weighted by 1/X7\n")
```

```{r}
# Imporve weighted least square regression further
# by removing two obvious high leverage points
lm.charlie.weighted.improved <- lm(Y ~ X7, data = data.charlie[-c(447, 75), ], 
                                   weights= weights[-c(447, 75)])
ShortSummary(lm.charlie.weighted.improved)
```

```{r}
# Plot all model's adjusted R squared bar plot
# Construct data frame for ploting
metrics.df <- bind_rows(glance(lm.charlie),
                        glance(lm.charlie.log),
                        glance(lm.charlie.sqrt),
                        glance(lm.charlie.weighted),
                        glance(lm.charlie.weighted.improved))
metrics.df <- transmute(metrics.df, 
                        adj.r.squared = round(adj.r.squared, 2), 
                        AIC = round(AIC, 2), BIC = round(BIC, 2))
metrics.df$model <- c("Y", "log(Y", "sqrt(Y)", "WLS", "WLS improved")
metrics.df<- melt(metrics.df, id.vars='model')
metrics.adj.r.squared <- slice(metrics.df, 1:5)
metrics.AIC.BIC <- slice(metrics.df, 6:15)

# Plot bar chart
g.adj.r.squared <- ggplot(metrics.adj.r.squared, aes(x = model, y = value)) +   
    geom_bar(aes(fill = variable), position = "dodge", stat="identity") + 
    geom_text(aes(x = model, y = value, label=value)) 
g.AIC.BIC <- ggplot(metrics.AIC.BIC, aes(x = model, y = value)) +   
    geom_bar(aes(fill = variable), position = "dodge", stat="identity") + 
    geom_text(aes(x = model, y = value, label=value)) 
grid.arrange(g.adj.r.squared, g.AIC.BIC)
```


# Experiment delta
```{r}
# Retrive delta data subset
# and keep relevant columns
data.delta <- data %>% 
    filter(label == "delta") %>%
    select(Y, X3, X8)
str(data.delta)
```

```{r}
# Initial graphical subset data exploration
BetterPairs(data.delta)
PlotTree(data.delta)
```

```{r}
# Visualise X3 - X8 2D Plane
data.delta <- mutate(data.delta, Y.label = as.factor(Y))
ggplot(data.delta, aes(x = X3, y = X8)) + 
    geom_point(data = data.delta, aes(pch = Y.label,  colour= Y.label)) 
```

```{r}
# Split data into training and testing subset for validation
set.seed(seed.num)
train <- sample(1:nrow(data.delta), nrow(data.delta)/2)
test <- (-train)
training <- data.delta[train, ]
testing <- data.delta[test, ]
testing.actual <- testing$Y.label
training.X <- select(training, X3, X8)
testing.X <- select(testing, X3, X8)
training.Y <- training$Y.label
```

```{r results='hide', message=FALSE, warning=FALSE}
# 10-fold cross validation on logisitic regression
# to find out the best formula explaining Y

# Set max polynomial degree to 10
poly.degree <- 10

# Initialise cross validation error
# for polynomial regression models (up to 10-degree)
cv.error.10 <- rep(0, poly.degree)

# Loop 10 times to fit data into 1-10 degree polynomial models
# and calculate and store 10-fold cross validation results
cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)
set.seed(seed.num)
for(i in 1:poly.degree){
    glm.delta <- glm(Y.label ~ polym(X3, X8, degree = i, raw = TRUE), 
                     data = data.delta, family = "binomial")
    cv.error.10[i] <- cv.glm(data.delta, glm.delta, cost, K = 10)$delta[1]
}

# Print the polynomial degree with minimum MSE
best.degree <- which.min(cv.error.10)

# Plot 10-fold cross validation results
qplot(1:length(cv.error.10), cv.error.10) + 
    geom_point(pch = 8) +
    geom_line() +
    xlab("Fitting Polynomial Degree") +
    ylab("Error Rate") +
    annotate("text", x = which.min(cv.error.10), y = min(cv.error.10), 
             label = "O", col = "red")

```

```{r}
# Fit logistic model
glm.delta <- glm(Y.label ~ X3 + X8, data = data.delta, family = binomial, subset = train)

# Colinearity Test
vif(glm.delta) > 5

# Calculate testing data prediction accuracy
testing.probs <- predict(glm.delta, testing, type = "response")
testing.pred <- rep(0, nrow(testing))
testing.pred[testing.probs > 0.5] <- 1
ClassificationReport(testing.pred, testing.actual)[1:2]
accuracy.glm <- ClassificationReport(testing.pred, testing.actual)[2] # 0.86
```

```{r}
# Fit LDA classifier 
# and calculate testing data prediction accuracy
lda.delta <- lda(Y.label ~ X3 + X8, data = data.delta, subset = train)
testing.pred <- predict(lda.delta, testing)$class
ClassificationReport(testing.pred, testing.actual)[1:2]
accuracy.lda <- ClassificationReport(testing.pred, testing.actual)[2] # 0.86
```

```{r}
# Fit QDA classifier 
# and calculate testing data prediction accuracy
qda.delta <- qda(Y.label ~ X3 + X8, data = data.delta, subset = train)
testing.pred <- predict(qda.delta, testing)$class
ClassificationReport(testing.pred, testing.actual)[1:2]
accuracy.qda <- ClassificationReport(testing.pred, testing.actual)[2] # 0.88
```
```{r}
# 10-fold cross validation on Knn
# to choose the best K 
set.seed(seed.num)
ctrl <- trainControl(method="cv") 
knnFit <- train(Y.label ~ X3 + X8, data = training, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)

# Results of kNN fit
plot(knnFit)
best.K <- knnFit$finalModel$k # 15
#best.K
```

```{r}
knnPredict <- predict(knnFit,newdata = testing)
ClassificationReport(knnPredict, testing.actual)[1:2]
accuracy.knn <- ClassificationReport(testing.pred, testing.actual)[2] # 0.84
accuracy.knn$Accuracy <- 0.84
```

```{r}
# Visualise all decision boundaries
ggplot(data.delta, aes(x=X3, y=X8) ) +
    geom_point(data = data.delta, aes(pch = Y.label,  colour= Y.label, alpha = 0.3)) + 
    geom_point(data = training, aes(pch = Y.label,  colour= Y.label)) + 
    LogisticRegressionDecisionBoundary(model = glm.delta) +
    LdaQdaDecisionBoundary(data.X = data.delta[, 2:3], model = lda.delta, colour = "red") + 
    LdaQdaDecisionBoundary(data.X = data.delta[, 2:3], model = qda.delta, colour = "blue") + 
    KnnDecisionBoundary(training.X, training.Y, k = best.K)
```

```{r}
# Plot all model's accuracy
# Construct data frame for ploting
metrics.df <- bind_rows(accuracy.glm,
                        accuracy.lda,
                        accuracy.qda,
                        accuracy.knn)
metrics.df <- transmute(metrics.df, 
                        Accuracy = round(Accuracy, 4))
metrics.df$model <- c("logistic", "LDA", "QDA", "KNN(15)")
metrics.df<- melt(metrics.df, id.vars='model')

# Plot bar chart
ggplot(metrics.df, aes(x = model, y = value)) +   
    geom_bar(aes(fill = variable), position = "dodge", stat="identity") + 
    geom_text(aes(x = model, y = value, label=value)) +
    ylab("Accuracy")
```

# Experiment echo
```{r}
# Retrive echo data subset
# and keep relevant columns
data.echo <- data %>%
    filter(label == "echo") %>% 
    select(Y, X3, X4, X5, X9)
str(data.echo)
```

```{r}
# Initial graphical data subset exploration
BetterPairs(data.echo)
PlotTree(data.echo)
```
```{r}
# Split data into training and testing subset for validation
set.seed(seed.num)
train <- sample(1:nrow(data.echo), nrow(data.echo)/2)
test <- (-train)
# divide by training and testing
training <- data.echo[train, ]
testing <- data.echo[test, ]
# divide by response and predictors
X <- model.matrix(Y ~ ., data = data.echo)[, -1]
Y <- data.echo$Y
```

```{r}
# Fit a multiple linear regression model
# using all variables as suggested by pairs plot
lm.echo <- lm(Y ~ X3 + X4 + X5 + X9, data = training)
ShortSummary(lm.echo)
par(mfrow = c(2, 2))
plot(lm.echo)
# Evaluate test MSE
lm.pred <- predict(lm.echo, newdata = testing)
MSE.my <- MSE(lm.pred, testing$Y) # 5.469873
```

```{r}
# Subset Selection Methods Using 10-fold Cross Validation

k <- 10 # k for K-fold CV
p <- 4 # number of predictors

# Randomly create k folds from data 
set.seed(seed.num)
folds <- sample(1:k, nrow(training), replace=TRUE)

# Initialise validation MSE matrix
cv.errors.best <- matrix(NA, k, p, dimnames=list(NULL, paste(1:p)))
cv.errors.fwd <- matrix(NA, k, p, dimnames=list(NULL, paste(1:p)))
cv.errors.bwd <- matrix(NA, k, p, dimnames=list(NULL, paste(1:p)))

# Loop k * p times to calculate validation MSE
# for best, forward and backward selection methods
for(j in 1:k){
  best.fit=regsubsets(Y ~ ., data = training[folds != j, ], nvmax= p)
  fwd.fit=regsubsets(Y ~ ., data = training[folds != j, ], nvmax= p,
                     method = "forward")
  bwd.fit=regsubsets(Y ~ ., data = training[folds != j, ], nvmax= p,
                     method = "backward")
  for(i in 1:p){
    pred <- PredictRegsubsets(best.fit, training[folds == j,], id = i)
    cv.errors.best[j, i] <- mean( (training$Y[folds == j] - pred) ^ 2)
    pred <- PredictRegsubsets(fwd.fit, training[folds == j,], id = i)
    cv.errors.fwd[j, i] <- mean( (training$Y[folds == j] - pred) ^ 2)
    pred <- PredictRegsubsets(bwd.fit, training[folds == j,], id = i)
    cv.errors.bwd[j, i] <- mean( (training$Y[folds == j] - pred) ^ 2)
    }
}

# Calculate average validation MSE and find number of predictors with
# smallest average validation MSE for best, forward and backward 
# selection methods
mean.cv.errors.best <- apply(cv.errors.best,2,mean)
p.best <- which.min(mean.cv.errors.best)
mean.cv.errors.fwd <- apply(cv.errors.fwd,2,mean)
p.fwd <- which.min(mean.cv.errors.fwd)
mean.cv.errors.bwd <- apply(cv.errors.bwd,2,mean)
p.bwd <- which.min(mean.cv.errors.bwd)

# Perform best subset selection on the full training subset
# and print coefficients
reg.best <- regsubsets(Y ~ ., data = training, nvmax = p)
coef(reg.best, p.best)
# Predict Y on testing subset and report test MSE
pred <- PredictRegsubsets(reg.best, testing, id = p.best)
MSE.best <- MSE(pred, testing$Y) # 5.469873


# Perform forward stepwise subset selection on the full training subset
# and print coefficients
reg.fwd <- regsubsets(Y ~ ., data = training, nvmax = p, 
                      method = "forward")
coef(reg.fwd, p.fwd)
# Predict Y on testing subset and report test MSE
pred <- PredictRegsubsets(reg.fwd, testing, id = p.fwd)
MSE.fwd <- MSE(pred, testing$Y) # 5.469873


# Perform backward stepwise subset selection on the full training subset
# and print coefficients
reg.bwd <- regsubsets(Y ~ ., data = training, nvmax = p, 
                      method = "backward")
coef(reg.bwd, p.bwd)
# Predict Y on testing subset and report test MSE
pred <- PredictRegsubsets(reg.bwd, testing, id = p.bwd)
MSE.bwd <- MSE(pred, testing$Y) # 5.469873
```

```{r}
# Fit ridge regression model
grid <- 10^seq(10, -2, length = 100)
ridge.model <- glmnet(X[train, ], Y[train], alpha = 0, lambda = grid,
                      thresh = 1e-12)

# 10-fold cross-validation
# to find out best lambda value
set.seed(seed.num)
cv.out <- cv.glmnet(X[train, ], Y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min # 0.7845004

# Show coefficients of final ridge regression model
out <- glmnet(X, Y, alpha = 0, lambda = grid)
ridge.coef <- predict(out, type = "coefficients", s = bestlam)
print(ridge.coef)

# Predict Y values on testing data
# and estimate test MSE
ridge.pred <- predict(ridge.model, s = bestlam, newx = X[test, ])
MSE.ridge <- MSE(ridge.pred, Y[test]) # 5.883374
```

```{r}
# Fit lasso regression model
lasso.model <- glmnet(X[train, ], Y[train], alpha = 1, lambda = grid,
                      thresh = 1e-12)

# 10-fold cross-validation
# to find out best lambda value
set.seed(seed.num)
cv.out <- cv.glmnet(X[train, ], Y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min # 0.03241565

# Show coefficients of final lasso regression model
out <- glmnet(X, Y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)
print(lasso.coef)

# Predict Y values on testing data
# and estimate test MSE
lasso.pred <- predict(lasso.model, s = bestlam, newx = X[test, ])
MSE.lasso <- MSE(lasso.pred, Y[test]) # 5.478034
```


```{r}
# Fit PCR model
set.seed(seed.num)
pcr.fit <- pcr(Y ~ ., data = training, scale = TRUE,
               validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP") # 4

# Compute test MSE
pcr.pred <- predict(pcr.fit, X[test, ], ncomp = 4)
MSE.pcr <- MSE(pcr.pred, Y[test]) # 5.469873
```

```{r}
# Fit PLS model
set.seed(seed.num)
pls.fit <- plsr(Y ~ ., data = training, scale = TRUE,
               validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP") # 2

# Compute test MSE
pls.pred <- predict(pls.fit, X[test, ], ncomp = 2)
MSE.pls <- MSE(pls.pred, Y[test]) # 5.49001
```

```{r}
# Plot all model's test MSE
# Construct data frame for ploting
metrics.df <- bind_rows(MSE.best,
                        MSE.fwd,
                        MSE.bwd,
                        MSE.ridge,
                        MSE.lasso,
                        MSE.pcr,
                        MSE.pls,
                        MSE.my)
metrics.df <- transmute(metrics.df, 
                        test.MSE = round(MSE, 2))
metrics.df$model <- c("best", "fwd", "bwd", "ridge", "lasso", "pcr(4)", "pls(2)", "My")
metrics.df<- melt(metrics.df, id.vars='model')

# Plot bar chart
ggplot(metrics.df, aes(x = model, y = value)) +   
    geom_bar(aes(fill = variable), position = "dodge", stat="identity") + 
    geom_text(aes(x = model, y = value, label = value)) +
    ylab("Test MSE")
```

# Experiment foxtrot
```{r}
# Retrive foxtrot data subset
# and keep relevant columns
data.foxtrot <- data %>% 
    filter(label == "foxtrot") %>% 
    select(Y, X3)
str(data.foxtrot)
```

```{r}
# Initial graphical data subset exploration
BetterPairs(data.foxtrot)
```

```{r}
# Fit a linear regression model 
# and evaluate the result
lm.foxtrot <- lm(Y ~ X3, data = data.foxtrot)
ShortSummary(lm.foxtrot)
par(mfrow = c(2, 2))
plot(lm.foxtrot)
```

```{r}
# Use 10 fold cross validation to find out
# best polynomial degree for polynomial regression
# Set max polynomial degree to 10
poly.degree <- 10

# Initialise cross validation error
# for polynomial regression models (up to 10-degree)
cv.error.10 <- rep(0, poly.degree)

# Loop 5 times to fit data into 1-10 degree polynomial models
# and calculate and store 10-fold cross validation results
set.seed(seed.num)
for(i in 1:poly.degree){
    lm.foxtrot <- glm(Y ~ poly(X3, i), data = data.foxtrot)
    cv.error.10[i] <- cv.glm(data.foxtrot, lm.foxtrot, K = 10)$delta[1]
}

# Print the polynomial degree with minimum MSE
best.degree <- which.min(cv.error.10) # 4

# Plot 10-fold cross validation results
qplot(1:length(cv.error.10), cv.error.10) + 
    geom_point(pch = 8) +
    geom_line() +
    xlab("Fitting Polynomial Degree") +
    ylab("MSE") +
    annotate("text", x = which.min(cv.error.10), y = min(cv.error.10), 
             label = "O", col = "red")
```

```{r}
# Fit best polynomial regression model and visualise
lm.foxtrot1 <- lm(Y ~ poly(X3, best.degree), data = data.foxtrot)
ShortSummary(lm.foxtrot1)
ggplot(data = data.foxtrot, aes(X3, Y)) +
    geom_point(colour = alpha("black", 0.3)) +
    geom_smooth(method = "lm", formula = y ~ x, col = "blue") +
    geom_smooth(method = "lm", formula = y ~ poly(x, best.degree, raw = TRUE), col = "red")
```
```{r}
# Plot all model's adjusted R squared, AIC an BIC bar plot
# Construct data frame for ploting
metrics.df <- bind_rows(glance(lm.foxtrot),
                        glance(lm.foxtrot1))
metrics.df <- transmute(metrics.df, 
                        adj.r.squared = round(adj.r.squared, 2), 
                        AIC = round(AIC, 2), BIC = round(BIC, 2))
metrics.df$model <- c("Linear", "Polynomial(4)")
metrics.df<- melt(metrics.df, id.vars='model')
metrics.adj.r.squared <- slice(metrics.df, 1:2)
metrics.AIC.BIC <- slice(metrics.df, 3:6)

# Plot bar chart
g.adj.r.squared <- ggplot(metrics.adj.r.squared, aes(x = model, y = value)) +   
    geom_bar(aes(fill = variable), position = "dodge", stat="identity") + 
    geom_text(aes(x = model, y = value, label=value)) 
g.AIC.BIC <- ggplot(metrics.AIC.BIC, aes(x = model, y = value)) +   
    geom_bar(aes(fill = variable), position = "dodge", stat="identity") + 
    geom_text(aes(x = model, y = value, label=value)) 
grid.arrange(g.adj.r.squared, g.AIC.BIC)
```

# Experiment golf 
```{r}
# Retrive golf data subset
# and keep relevant columns
data.golf <- data %>%
    filter(label == "golf") %>% 
    select(Y, X4)
str(data.golf)
```

```{r}
# Initial graphical data subset exploration
BetterPairs(data.golf)
```

```{r}
# Fit a linear regression model 
# and evaluate the result
lm.golf <- lm(Y ~ X4, data = data.golf)
ShortSummary(lm.golf)
par(mfrow = c(2, 2))
plot(lm.golf)
```

```{r}
# Detect outliers
outlierTest(lm.golf)
```

```{r}
# No severe outlier
# but there is a high leverage point
# Remove high leverage point and 
# refit the model
data.golf1 <- data.golf[-1, ]
lm.golf1 <- lm(Y ~ X4, data = data.golf1)
ShortSummary(lm.golf1)
par(mfrow = c(2, 2))
plot(lm.golf1)
```

```{r}
# Plot linear regression line for original data
# and data without high leverage point
ggplot(data.golf, aes(X4, Y)) +
    geom_point(data = data.golf, colour = alpha("black", 0.3)) +
    geom_smooth(data = data.golf, formula = y ~ x, method = "lm",
                colour = "red", fill = "red", alpha = 0.3) +
    geom_smooth(data = data.golf1, formula = y ~ x, method = "lm",
                colour = "blue", fill = "blue", alpha = 0.3) 
```

```{r}
# Plot all model's adjusted R squared, AIC and BIC bar plot
# Construct data frame for ploting
metrics.df <- bind_rows(glance(lm.golf),
                        glance(lm.golf1))
metrics.df <- transmute(metrics.df, 
                        adj.r.squared = round(adj.r.squared, 4), 
                        AIC = round(AIC, 2), BIC = round(BIC, 2))
metrics.df$model <- c("Original", "High Leverage Point Removed")
metrics.df<- melt(metrics.df, id.vars='model')
metrics.adj.r.squared <- slice(metrics.df, 1:2)
metrics.AIC.BIC <- slice(metrics.df, 3:6)

# Plot bar chart
g.adj.r.squared <- ggplot(metrics.adj.r.squared, aes(x = model, y = value)) +   
    geom_bar(aes(fill = variable), position = "dodge", stat="identity") + 
    geom_text(aes(x = model, y = value, label=value)) 
g.AIC.BIC <- ggplot(metrics.AIC.BIC, aes(x = model, y = value)) +   
    geom_bar(aes(fill = variable), position = "dodge", stat="identity") + 
    geom_text(aes(x = model, y = value, label=value)) 
grid.arrange(g.adj.r.squared, g.AIC.BIC)
```

# Experiment hotel
```{r}
# Retrive hotel data subset
# and keep relevant columns
data.hotel <- data %>% 
    filter(label == "hotel") %>%
    select(Y, X4)
str(data.hotel)
```

```{r}
# Initial graphical data subset exploration
BetterPairs(data.hotel)
```

```{r}
# Evaluate linear regression model of original data
lm.hotel <- lm(Y~X4, data = data.hotel)
ShortSummary(lm.hotel)
par(mfrow = c(2, 2))
plot(lm.hotel)
```

```{r}
# Detect outliers
outlierTest(lm.hotel)
outliers <- which(data.hotel$Y > 900)
```

```{r}
# Create data with outlier 1
# and evaluate its linear regression model
data.hotel.with.outlier1 <- data.hotel[-outliers[2], ]
lm.hotel.with.outlier1 <- lm(Y~X4, data = data.hotel.with.outlier1)
ShortSummary(lm.hotel.with.outlier1)
par(mfrow = c(2, 2))
plot(lm.hotel.with.outlier1)
```

```{r}
# Create data with outlier 2
# and evaluate its linear regression model
data.hotel.with.outlier2 <- data.hotel[-outliers[1], ]
lm.hotel.with.outlier2 <- lm(Y~X4, data = data.hotel.with.outlier2)
ShortSummary(lm.hotel.with.outlier2)
par(mfrow = c(2, 2))
plot(lm.hotel.with.outlier2)
```

```{r}
# Create data without outiers
# and evaluate its linear regression model
data.hotel.no.outlier <- data.hotel[-outliers, ]
lm.hotel.no.outlier <- lm(Y~X4, data = data.hotel.no.outlier)
ShortSummary(lm.hotel.no.outlier)
par(mfrow = c(2, 2))
plot(lm.hotel.no.outlier)
```

```{r}
# Test which outlier has server impact by
# ploting linear regression line for original data, data with outlier1,
# data with outlier1, data without outliers
ggplot(data.hotel.no.outlier, aes(X4, Y)) +
    geom_point(data = data.hotel.no.outlier, colour = alpha("black", 0.3)) +
    geom_smooth(data = data.hotel, formula = y ~ x, method = "lm",
                colour = 1, fill = 1, alpha = 0.05) +
    geom_smooth(data = data.hotel.with.outlier1, formula = y ~ x, method = "lm",
                colour = 2, fill = 2, alpha = 0.05) +
    geom_smooth(data = data.hotel.with.outlier2, formula = y ~ x, method = "lm",
                colour = 3, fill = 3, alpha = 0.05) +
    geom_smooth(data = data.hotel.no.outlier, formula = y ~ x, method = "lm",
                colour = 4, fill = 4, alpha = 0.05)
```

```{r}
# Plot all model's adjusted R squared, AIC and BIC bar plot
# Construct data frame for ploting
metrics.df <- bind_rows(glance(lm.hotel),
                        glance(lm.hotel.with.outlier1),
                        glance(lm.hotel.with.outlier2),
                        glance(lm.hotel.no.outlier))
metrics.df <- transmute(metrics.df, 
                        adj.r.squared = round(adj.r.squared, 4), 
                        AIC = round(AIC, 2), BIC = round(BIC, 2))
metrics.df$model <- c("Original", "w/ Outerlier1", "w/ Outlier2", "w/o Outliers")
metrics.df<- melt(metrics.df, id.vars='model')
metrics.adj.r.squared <- slice(metrics.df, 1:4)
metrics.AIC.BIC <- slice(metrics.df, 5:12)

# Plot bar chart
g.adj.r.squared <- ggplot(metrics.adj.r.squared, aes(x = model, y = value)) +   
    geom_bar(aes(fill = variable), position = "dodge", stat="identity") + 
    geom_text(aes(x = model, y = value, label=value)) 
g.AIC.BIC <- ggplot(metrics.AIC.BIC, aes(x = model, y = value)) +   
    geom_bar(aes(fill = variable), position = "dodge", stat="identity") + 
    geom_text(aes(x = model, y = value, label=value)) 
grid.arrange(g.adj.r.squared, g.AIC.BIC)
```

# myutils.R: Some Helper Functions
```{r}
# There are some helper functions

ShortSummary <- function(model){
    # Print a tidy data frame version of lm model summary
    #
    # Args:
    #   model: A fitted linear model to be summarised
    #
    # Returns:
    #   None
    
    # Require packages
    require(broom)
    
    # Tidy and print model summary
    estimates <- tidy(model)
    metrics <- glance(model)
    print(estimates)
    print(metrics)
}

BetterPairs <- function(data) {
    # Create a customised pairs plot showing scatter plots,
    # histograms and correlation coefficients
    #
    # Args:
    #   data: Data frame to be plotted
    #
    # Returns:
    #   A customised pairs plot
    
    return(pairs(data,
                 # Create function to plot pairwise scatterplot and draw linear regression line
                 upper.panel=panel.regression <- function(x, y, col = rgb(0, 0, 0, 0.3), bg = NA, pch = 19, 
                                                              cex = 1, col.regres = "blue", col.smooth = "red", 
                                                              span = 2/3, iter = 3, ...) { 
                     points(x, y, pch = pch, col = col, bg = bg, cex = cex) 
                     ok <- is.finite(x) & is.finite(y) 
                     if (any(ok)) {
                         abline(stats::lm(y[ok] ~ x[ok]), col = col.regres, ...)
                         lines(stats::lowess(x[ok], y[ok], f = span, iter = iter), col = col.smooth, ...)
                     }
                 }, 
                 # Create function to compute and print correlation
                 lower.panel=panel.cor <- function(x, y, digits=2, prefix="", cex.cor) {
                     # Set user coordinates of plotting region
                     usr <- par("usr"); on.exit(par(usr)) 
                     par(usr = c(0, 1, 0, 1)) 
                     r <- abs(cor(x, y)) 
                     txt <- format(c(r, 0.123456789), digits=digits)[1] 
                     txt <- paste(prefix, txt, sep="") 
                     if(missing(cex.cor)) cex <- 0.8/strwidth(txt) 
                     
                     test <- cor.test(x,y) 
                     # borrowed from printCoefmat
                     Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, 
                                      cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                                      symbols = c("***", "**", "*", ".", " ")) 
                     
                     text(.5, .5, txt, cex = cex * sqrt(r) * 1.2) 
                     text(.8, .8, Signif, cex=cex, col=2) 
                 },
                 # Create function to make histogram with density superimposed
                 diag.panel=panel.hist <- function(x, ...) {
                     # Set user coordinates of plotting region
                     usr <- par("usr"); on.exit(par(usr))
                     par(usr = c(usr[1:2], 0, 1.5))
                     # Do not start new plot
                     par(new=TRUE)
                     # Draw histogram
                     hist(x, prob=TRUE, axes=FALSE, xlab="", ylab="",
                          main="", col="cyan")
                     # Add density curve
                     lines(density(x, na.rm=TRUE))
                     # Add rug representation
                     rug(x)
                 }))
}


#input actual & predicted vectors or actual vs predicted confusion matrix 
ClassificationReport <- function(actual=NULL, predicted=NULL, cm=NULL){
    # Print a classification report consisting of confusion matrix,
    # accuracy, precision, recall and F1 scores
    #
    # Args:
    #   actual: Testing labels
    #   predicted: Predicted testing labels
    #   cm: Confustion matrix
    #
    # Returns:
    #   A classification report in the form of list
    
    if(is.null(cm)) {
        naVals <- union(which(is.na(actual)), which(is.na(predicted)))
        if(length(naVals) > 0) {
            actual <- actual[-naVals]
            predicted <- predicted[-naVals]
        }
        f <- factor(union(unique(actual), unique(predicted)))
        actual <- factor(actual, levels = levels(f))
        predicted <- factor(predicted, levels = levels(f))
        cm <- as.matrix(table(Actual=actual, Predicted=predicted))
    }
    
    n <- sum(cm) # number of instances
    nc <- nrow(cm) # number of classes
    diag <- diag(cm) # number of correctly classified instances per class 
    rowsums <- apply(cm, 1, sum) # number of instances per class
    colsums <- apply(cm, 2, sum) # number of predictions per class
    p <- rowsums / n # distribution of instances over the classes
    q <- colsums / n # distribution of instances over the predicted classes
    
    #accuracy
    accuracy <- sum(diag) / n
    
    #per class prf
    recall <- diag / rowsums
    precision <- diag / colsums
    f1 <- 2 * precision * recall / (precision + recall)
    
    
    classNames <- names(diag)
    if(is.null(classNames)) {
        classNames <- paste("C",(1:nc),sep="")
    }
    
    metrics <- rbind(
        Accuracy = accuracy,
        Precision = precision,
        Recall = recall,
        F1 = f1)
    
    colnames(metrics) <- classNames
    
    return(list(ConfusionMatrix = cm, Accuracy = accuracy, Metrics = metrics))
}

LogisticRegressionDecisionBoundary <- function(model, colour = "black", ...){
    # Draw decision boundary for logistic regression models
    #
    # Args:
    #   model: A logisitic regression model
    #   colour: Colour of decision boundary line
    #
    # Returns:
    #   A ggplot line
    
    # Compute slope and intercept from model coefficient
    slope <- coef(model)[2]/(-coef(model)[3])
    intercept <- coef(model)[1]/(-coef(model)[3])
    
    # Return plot
    return(
        geom_abline(slope = slope, intercept = intercept, colour = colour)
    )
}


LdaQdaDecisionBoundary <- function(data.X, model, colour = "red2"){
    # Draw decision boundary for LDA or QDA models
    #
    # Args:
    #   data: Training data variables
    #   model: A LDA or QDA model
    #   colour: Colour of decision boundary
    #
    # Returns:
    #   A ggplot contour  

    # Require packages
    require(MASS)
    require(tidyverse)
    
    # Create test data covering a grid
    xlim <- range(data.X[, 1])
    x <- seq(xlim[1], xlim[2], length.out = 300)  
    ylim <- range(data.X[, 2])
    y <- seq(ylim[1], ylim[2], length.out = 300)
    test.grid <- expand.grid(list(x,y))
    names(test.grid) <- names(data.X) # match column names for prediction

    # Prediction for that grid
    preds <-predict(model, newdata = test.grid)
    predclass <- preds$class
    
    # Data structure for plotting
    names(test.grid) <- c("x", "y")
    df <- cbind(test.grid, predclass)
    df$classnum <- as.numeric(df$predclass)
    
    # Return plot
    return(geom_contour(data=df, 
                        aes(x = x, y = y, z = classnum), 
                        colour = colour, 
                        breaks= c(1.5,2.5))
    )
}

KnnDecisionBoundary <- function(training.X, training.Y, k = 1){
    # Draw decision boundary for KNN models
    #
    # Args:
    #   training.X: Training data variables
    #   training.Y: Training data lables
    #   k: Number of neighbors
    #
    # Returns:
    #   A ggplot contour     
    
    # Require packages
    require(tidyverse)
    
    # Create test data covering a grid
    test.grid <- expand.grid(x=seq(min(training.X[,1]), max(training.X[,1]),
                              by=0.1),
                        y=seq(min(training.X[,2]), max(training.X[,2]), 
                              by=0.1))
    
    # Classification and prediction for that grid
    classif <- knn(training.X, test.grid, training.Y, k = k, prob=TRUE)
    prob <- attr(classif, "prob")
    
    # Data structure for plotting
    df <- bind_rows(mutate(test.grid,
                           prob=prob,
                           cls=1,
                           prob_cls=ifelse(classif==cls,
                                           1, 0)),
                    mutate(test.grid,
                           prob=prob,
                           cls=0,
                           prob_cls=ifelse(classif==cls,
                                           1, 0)))
    df <- mutate(df, Y.label = as.factor(cls))
    
    # Return plot
    return(geom_contour(data = df,
                        aes(x = x, y = y, z = prob_cls, group = Y.label, color = Y.label),
                        bins=2)
    )
}


PlotTree <- function(data){
    # Plot a full tree model
    #
    # Args:
    #   data: A data frame
    #
    # Returns:
    #   A text tree model
    
    # Require packages
    require(tree)
    
    # Create and print a full tree model
    model <- tree(Y ~ ., data = data)
    plot(model)
    text(model)
}

PredictRegsubsets <- function(object, newdata, id, ...){
    # Predict new data for a subset model
    #
    # Args:
    #   object: A fitted subset selection model
    #   data: New data to be predicted
    #   id: Number of precdictors
    #
    # Returns:
    #   A matrix of computed prediction values
    
    # Require packages
    require(leaps)
    
    form <- as.formula(object$call[[2]])
    mat <- model.matrix(form, newdata)
    coefi <- coef(object, id = id)
    xvars <- names(coefi)
    mat[, xvars] %*% coefi
}

MSE <- function(pred, actual){
    # Calculate MSe
    #
    # Args:
    #   pred: Predication values
    #   acutal: Actual values
    #
    # Returns:
    #   A MSE value
    
    return(
        list(MSE = mean((pred - actual) ^ 2))
    )
}
```

